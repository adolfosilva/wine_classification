{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d628b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f494a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0efa427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = LabelEncoder().fit_transform(df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138e198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('type')\n",
    "X = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30c233",
   "metadata": {},
   "source": [
    "Define some utility functions and the default cross validation and scoring functions to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff682d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def print_scores(results, sorted_by=None, show_timings=False):\n",
    "    if sorted_by:\n",
    "        results = sorted(results, key=lambda x: statistics.mean(x[1][sorted_by]), reverse=True)\n",
    "\n",
    "    for model, v in results:\n",
    "        print(f\"{model}:\")\n",
    "        for metric, values in v.items():\n",
    "            if not show_timings and not metric.startswith(\"test_\"): continue\n",
    "            print(f\"{metric}: {statistics.mean(values):.6f}\")\n",
    "\n",
    "def scores_to_csv(results, round_to = 5):   \n",
    "    scores = defaultdict(list)\n",
    "    for model, v in results:\n",
    "        scores[\"model\"].append(model)\n",
    "        for metric, values in v.items():\n",
    "            if not metric.startswith(\"test_\"): continue\n",
    "            scores[metric].append(round(statistics.mean(values), round_to))\n",
    "    return pd.DataFrame(data=scores).to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069594bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "scoring = ['f1_weighted', 'roc_auc_ovr_weighted','balanced_accuracy', 'matthews_corrcoef']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437d7f1",
   "metadata": {},
   "source": [
    "Basic Decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1f541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier:\n",
      "test_f1_weighted: 0.986412\n",
      "test_roc_auc_ovr_weighted: 0.982428\n",
      "test_balanced_accuracy: 0.982137\n",
      "test_matthews_corrcoef: 0.963454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "scores = cross_validate(model, X, y, scoring=scoring, n_jobs=-1, cv=cv)\n",
    "print_scores([(model.__class__.__name__, scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe44eff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GradientBoostingClassifier.__init__() got an unexpected keyword argument 'l2_regularization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSVC, SVC\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n\u001b[1;32m     10\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m     DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     13\u001b[0m     GaussianNB(),\n\u001b[1;32m     14\u001b[0m     KNeighborsClassifier(),\n\u001b[1;32m     15\u001b[0m     MLPClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     16\u001b[0m     LinearDiscriminantAnalysis(),\n\u001b[1;32m     17\u001b[0m     SVC(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m,probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m     RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m     ExtraTreesClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m---> 20\u001b[0m     GradientBoostingClassifier(\n\u001b[1;32m     21\u001b[0m         l2_regularization\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.51153447372028e-10\u001b[39m,\n\u001b[1;32m     22\u001b[0m         lear\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m         max_bins\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m     24\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m         max_leaf_nodes\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m,\n\u001b[1;32m     26\u001b[0m         min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m,\n\u001b[1;32m     27\u001b[0m         n_iter_no_change\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m         scoring\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m         tol\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-07\u001b[39m)\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m classifiers:\n",
      "\u001b[0;31mTypeError\u001b[0m: GradientBoostingClassifier.__init__() got an unexpected keyword argument 'l2_regularization'"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=0, multi_class='ovr'),\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier(random_state=0),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    SVC(random_state=0, kernel='linear',probability=True),\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    ExtraTreesClassifier(random_state=0)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model in classifiers:\n",
    "    model_name = model.__class__.__name__\n",
    "    scores = cross_validate(model, X, y, scoring=scoring, n_jobs=-1, cv=cv)\n",
    "    results.append([model_name, scores])\n",
    "\n",
    "#print_scores(results, 'test_f1_weighted')\n",
    "csv = scores_to_csv(results)\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32aa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2023-03-01 15:52:03,973:Client-AutoMLSMBO(1)::wine] Could not find meta-data directory /home/asilva/.local/lib/python3.10/site-packages/autosklearn/metalearning/files/matthews_corrcoef_binary.classification_dense\n",
      "auto-sklearn results:\n",
      "  Dataset name: wine\n",
      "  Metric: matthews_corrcoef\n",
      "  Best validation score: 0.991713\n",
      "  Number of target algorithm runs: 152\n",
      "  Number of successful target algorithm runs: 123\n",
      "  Number of crashed target algorithm runs: 11\n",
      "  Number of target algorithms that exceeded the time limit: 18\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "          rank  ensemble_weight               type      cost   duration\n",
      "model_id                                                               \n",
      "144          1             0.20  gradient_boosting  0.008287   6.761546\n",
      "133          2             0.06                lda  0.008313  25.652464\n",
      "132          3             0.02  gradient_boosting  0.009539  10.561324\n",
      "149          4             0.02  gradient_boosting  0.010365   8.346216\n",
      "129          5             0.04  gradient_boosting  0.010374   8.027264\n",
      "20           6             0.02                lda  0.010388   6.140369\n",
      "51           7             0.02           adaboost  0.013704  10.932482\n",
      "121          8             0.54           adaboost  0.014546  17.092373\n",
      "71           9             0.08           adaboost  0.015358  21.510676\n",
      "CPU times: user 3min 41s, sys: 11 s, total: 3min 52s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "import autosklearn.classification\n",
    "from autosklearn.metrics import f1_weighted, log_loss\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "def roc_auc_ovr_weighted(y_true, y_score):\n",
    "    return roc_auc_score(y_true, y_score, average=\"weighted\", multi_class=\"ovr\")\n",
    "\n",
    "rocauc = autosklearn.metrics.make_scorer('roc_auc_ovr_weighted', roc_auc_ovr_weighted)                                                                                            \n",
    "\n",
    "mcc = autosklearn.metrics.make_scorer('matthews_corrcoef', matthews_corrcoef)\n",
    "\n",
    "#automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "#    metric=mcc,\n",
    "#    time_left_for_this_task=240,\n",
    "#    per_run_time_limit=30,\n",
    "#    n_jobs=-1,\n",
    "    #n_jobs=16,\n",
    "    #memory_limit=2048,\n",
    "#    tmp_folder=\"/tmp/autosklearn_resampling_example_tmp\",\n",
    "#    disable_evaluator_output=False,\n",
    "#    resampling_strategy=cv,\n",
    "    #scoring_functions=[f1_weighted, rocauc, log_loss]\n",
    "#)\n",
    "\n",
    "import autosklearn.classification\n",
    "import sklearn.metrics\n",
    "\n",
    "#scorer = autosklearn.metrics.make_scorer('f1_score', sklearn.metrics.f1_score, pos_label=0)\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    metric=mcc,\n",
    "    time_left_for_this_task=240,\n",
    "    per_run_time_limit=30,\n",
    "    n_jobs=-1,\n",
    "    #memory_limit=2048,\n",
    "    tmp_folder=\"/tmp/autosklearn_resampling_example_tmp\",\n",
    "    disable_evaluator_output=False,\n",
    "    resampling_strategy=\"cv\",\n",
    "    resampling_strategy_arguments={\"folds\": 10},\n",
    ")\n",
    "\n",
    "#automl = AutoSklearn2Classifier(resampling_strategy=cv)\n",
    "\n",
    "automl.fit(X, y, dataset_name=\"wine\")\n",
    "\n",
    "print(automl.sprint_statistics())\n",
    "print(automl.leaderboard())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6316a611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 24,\n",
       " 'rank': 2,\n",
       " 'cost': 0.0033861782361089723,\n",
       " 'ensemble_weight': 0.04,\n",
       " 'voting_model': VotingClassifier(estimators=None, voting='soft'),\n",
       " 'estimators': [{'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice at 0x7f12181219f0>,\n",
       "   'balancing': Balancing(random_state=1, strategy='weighting'),\n",
       "   'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice at 0x7f118f8f3910>,\n",
       "   'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice at 0x7f118f8f27d0>,\n",
       "   'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
       "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
       "                        random_state=1, warm_start=True)},\n",
       "  {'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice at 0x7f11b06bb670>,\n",
       "   'balancing': Balancing(random_state=1, strategy='weighting'),\n",
       "   'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice at 0x7f11f828df60>,\n",
       "   'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice at 0x7f11f828e3b0>,\n",
       "   'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
       "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
       "                        random_state=1, warm_start=True)},\n",
       "  {'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice at 0x7f12006c3be0>,\n",
       "   'balancing': Balancing(random_state=1, strategy='weighting'),\n",
       "   'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice at 0x7f11f86a08e0>,\n",
       "   'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice at 0x7f11f86a37f0>,\n",
       "   'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
       "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
       "                        random_state=1, warm_start=True)},\n",
       "  {'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice at 0x7f1268845b40>,\n",
       "   'balancing': Balancing(random_state=1, strategy='weighting'),\n",
       "   'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice at 0x7f12247a9180>,\n",
       "   'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice at 0x7f12247a8b50>,\n",
       "   'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
       "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
       "                        random_state=1, warm_start=True)},\n",
       "  {'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice at 0x7f11f85e4a00>,\n",
       "   'balancing': Balancing(random_state=1, strategy='weighting'),\n",
       "   'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice at 0x7f11f85e6350>,\n",
       "   'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice at 0x7f11f85e6470>,\n",
       "   'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
       "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
       "                        random_state=1, warm_start=True)}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.show_models()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3621c117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/asilva/.local/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier:\n",
      "test_f1_weighted: 0.996353\n",
      "test_roc_auc_ovr_weighted: 0.998909\n",
      "test_balanced_accuracy: 0.994075\n",
      "test_matthews_corrcoef: 0.990185\n",
      "HistGradientBoostingClassifier:\n",
      "test_f1_weighted: 0.996042\n",
      "test_roc_auc_ovr_weighted: 0.998866\n",
      "test_balanced_accuracy: 0.993309\n",
      "test_matthews_corrcoef: 0.989353\n",
      "HistGradientBoostingClassifier:\n",
      "test_f1_weighted: 0.995943\n",
      "test_roc_auc_ovr_weighted: 0.998842\n",
      "test_balanced_accuracy: 0.993801\n",
      "test_matthews_corrcoef: 0.989083\n",
      "AdaBoostClassifier:\n",
      "test_f1_weighted: 0.995888\n",
      "test_roc_auc_ovr_weighted: 0.998644\n",
      "test_balanced_accuracy: 0.993348\n",
      "test_matthews_corrcoef: 0.988937\n",
      "ExtraTreesClassifier:\n",
      "test_f1_weighted: 0.995784\n",
      "test_roc_auc_ovr_weighted: 0.999195\n",
      "test_balanced_accuracy: 0.992717\n",
      "test_matthews_corrcoef: 0.988651\n",
      "model,test_f1_weighted,test_roc_auc_ovr_weighted,test_balanced_accuracy,test_matthews_corrcoef\n",
      "HistGradientBoostingClassifier,0.99594,0.99884,0.9938,0.98908\n",
      "HistGradientBoostingClassifier,0.99604,0.99887,0.99331,0.98935\n",
      "HistGradientBoostingClassifier,0.99635,0.99891,0.99407,0.99018\n",
      "AdaBoostClassifier,0.99589,0.99864,0.99335,0.98894\n",
      "ExtraTreesClassifier,0.99578,0.9992,0.99272,0.98865\n",
      "\n",
      "CPU times: user 293 ms, sys: 269 ms, total: 563 ms\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "#v = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#coring = ['f1','precision_macro', 'recall_macro', 'balanced_accuracy']\n",
    "\n",
    "classifiers = [\n",
    "    HistGradientBoostingClassifier(early_stopping=True,\n",
    "                                  l2_regularization=0.0002845813444661551,\n",
    "                                  learning_rate=0.5763871990263213, max_iter=32,\n",
    "                                  max_leaf_nodes=313, min_samples_leaf=200,\n",
    "                                  random_state=1,\n",
    "                                  validation_fraction=0.0984819561748849,\n",
    "                                  warm_start=True),\n",
    "    \n",
    "    HistGradientBoostingClassifier(early_stopping=True, l2_regularization=1e-10,\n",
    "                                  learning_rate=0.061183590203803505, loss='auto',\n",
    "                                  max_iter=512, max_leaf_nodes=14,\n",
    "                                  min_samples_leaf=31, n_iter_no_change=9,\n",
    "                                  random_state=1, validation_fraction=None,\n",
    "                                  warm_start=True),\n",
    "    \n",
    "    HistGradientBoostingClassifier(early_stopping=True,\n",
    "                                  l2_regularization=0.0024034282201116797,\n",
    "                                  learning_rate=0.49328674977232706, max_iter=128,\n",
    "                                  max_leaf_nodes=9, min_samples_leaf=95,\n",
    "                                  n_iter_no_change=2, random_state=1,\n",
    "                                  validation_fraction=None, warm_start=True),\n",
    "    \n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
    "                      learning_rate=0.949552173721954, n_estimators=256,\n",
    "                      random_state=1),\n",
    "    \n",
    "    ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=2,\n",
    "                        min_samples_split=6, n_estimators=512, n_jobs=1,\n",
    "                        random_state=1, warm_start=True)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model in classifiers:\n",
    "    model_name = model.__class__.__name__\n",
    "    scores = cross_validate(model, X, y, scoring=scoring, n_jobs=-1, cv=cv)\n",
    "    results.append([model_name, scores])\n",
    "\n",
    "print_scores(results, 'test_f1_weighted')\n",
    "csv = scores_to_csv(results)\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9711fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(automl.cv_results_).to_csv(\"automl2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cb8d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'balancing:strategy': 'none',\n",
      " 'classifier:__choice__': 'gradient_boosting',\n",
      " 'classifier:gradient_boosting:early_stop': 'train',\n",
      " 'classifier:gradient_boosting:l2_regularization': 2.51153447372028e-10,\n",
      " 'classifier:gradient_boosting:learning_rate': 0.08568989054310866,\n",
      " 'classifier:gradient_boosting:loss': 'auto',\n",
      " 'classifier:gradient_boosting:max_bins': 255,\n",
      " 'classifier:gradient_boosting:max_depth': 'None',\n",
      " 'classifier:gradient_boosting:max_leaf_nodes': 21,\n",
      " 'classifier:gradient_boosting:min_samples_leaf': 77,\n",
      " 'classifier:gradient_boosting:n_iter_no_change': 1,\n",
      " 'classifier:gradient_boosting:scoring': 'loss',\n",
      " 'classifier:gradient_boosting:tol': 1e-07,\n",
      " 'data_preprocessor:__choice__': 'feature_type',\n",
      " 'data_preprocessor:feature_type:numerical_transformer:imputation:strategy': 'median',\n",
      " 'data_preprocessor:feature_type:numerical_transformer:rescaling:__choice__': 'minmax',\n",
      " 'feature_preprocessor:__choice__': 'polynomial',\n",
      " 'feature_preprocessor:polynomial:degree': 2,\n",
      " 'feature_preprocessor:polynomial:include_bias': 'False',\n",
      " 'feature_preprocessor:polynomial:interaction_only': 'True'}\n"
     ]
    }
   ],
   "source": [
    "pprint({'balancing:strategy': 'none', 'classifier:__choice__': 'gradient_boosting', 'data_preprocessor:__choice__': 'feature_type', 'feature_preprocessor:__choice__': 'polynomial', 'classifier:gradient_boosting:early_stop': 'train', 'classifier:gradient_boosting:l2_regularization': 2.51153447372028e-10, 'classifier:gradient_boosting:learning_rate': 0.08568989054310866, 'classifier:gradient_boosting:loss': 'auto', 'classifier:gradient_boosting:max_bins': 255, 'classifier:gradient_boosting:max_depth': 'None', 'classifier:gradient_boosting:max_leaf_nodes': 21, 'classifier:gradient_boosting:min_samples_leaf': 77, 'classifier:gradient_boosting:scoring': 'loss', 'classifier:gradient_boosting:tol': 1e-07, 'data_preprocessor:feature_type:numerical_transformer:imputation:strategy': 'median', 'data_preprocessor:feature_type:numerical_transformer:rescaling:__choice__': 'minmax', 'feature_preprocessor:polynomial:degree': 2, 'feature_preprocessor:polynomial:include_bias': 'False', 'feature_preprocessor:polynomial:interaction_only': 'True', 'classifier:gradient_boosting:n_iter_no_change': 1})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
